{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import value_and_grad,jit,vmap,grad\n",
    "import jax.numpy as jnp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.key(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'jax.interpreters.pxla' has no attribute 'ShardedDeviceArray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mjnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m value_and_grad, grad\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01moptax\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLSTM\u001b[39;00m():\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_size: \u001b[38;5;28mint\u001b[39m, hidden_size: \u001b[38;5;28mint\u001b[39m, rng):\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.11/site-packages/optax/__init__.py:17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2019 DeepMind Technologies Limited. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Optax: composable gradient processing and optimization, in JAX.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptax\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m experimental\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malias\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m adabelief\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malias\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m adafactor\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.11/site-packages/optax/experimental/__init__.py:20\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2021 DeepMind Technologies Limited. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Experimental features in Optax.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03mFeatures may be removed or modified at any time.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomplex_valued\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m split_real_and_imaginary\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomplex_valued\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SplitRealAndImaginaryState\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextra_args\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GradientTransformationWithExtraArgs\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.11/site-packages/optax/_src/experimental/complex_valued.py:32\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Complex-valued optimization.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03mWhen using `split_real_and_imaginary` to wrap an optimizer, we split the complex\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03mSee details at https://github.com/deepmind/optax/issues/196\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NamedTuple, Union\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchex\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mjnp\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.11/site-packages/chex/__init__.py:17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 DeepMind Technologies Limited. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Chex: Testing made fun, in JAX!\"\"\"\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mchex\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01masserts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m assert_axis_dimension\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mchex\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01masserts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m assert_axis_dimension_comparator\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mchex\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01masserts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m assert_axis_dimension_gt\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.11/site-packages/chex/_src/asserts.py:26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01munittest\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munittest\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mock\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mchex\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m asserts_internal \u001b[38;5;28;01mas\u001b[39;00m _ai\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mchex\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pytypes\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.11/site-packages/chex/_src/asserts_internal.py:34\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Sequence, Union, Callable, Optional, Set, Tuple, Type\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mabsl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logging\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mchex\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pytypes\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m checkify\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.11/site-packages/chex/_src/pytypes.py:25\u001b[0m\n\u001b[1;32m     23\u001b[0m ArrayBatched \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39minterpreters\u001b[38;5;241m.\u001b[39mbatching\u001b[38;5;241m.\u001b[39mBatchTracer\n\u001b[1;32m     24\u001b[0m ArrayNumpy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray\n\u001b[0;32m---> 25\u001b[0m ArraySharded \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39minterpreters\u001b[38;5;241m.\u001b[39mpxla\u001b[38;5;241m.\u001b[39mShardedDeviceArray\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Use this type for type annotation. For instance checking,  use\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# `isinstance(x, jax.DeviceArray)`.\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# `jax.interpreters.xla._DeviceArray` appears in jax > 0.2.5\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(jax\u001b[38;5;241m.\u001b[39minterpreters\u001b[38;5;241m.\u001b[39mxla, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_DeviceArray\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'jax.interpreters.pxla' has no attribute 'ShardedDeviceArray'"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import value_and_grad, grad\n",
    "import optax\n",
    "\n",
    "class LSTM():\n",
    "    def __init__(self, input_size: int, hidden_size: int, rng):\n",
    "        \"\"\"\n",
    "        Constructor for the LSTM class.\n",
    "        -------------------------------\n",
    "        Parameters:\n",
    "        input_size (int): The number of input features.\n",
    "        hidden_size (int): The number of units in the hidden state.\n",
    "        rng (jax.random.PRNGKey): Random number generator key.\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Properly handle random key splitting\n",
    "        keys = jax.random.split(rng, 13)  # Split into 13 keys for all parameters\n",
    "        \n",
    "        # Initialize weights and biases for input gate\n",
    "        self.Wii = jax.random.normal(keys[0], (input_size, hidden_size)) * 0.01\n",
    "        self.Whi = jax.random.normal(keys[1], (hidden_size, hidden_size)) * 0.01\n",
    "        self.bi = jax.random.normal(keys[2], (hidden_size,)) * 0.01\n",
    "        \n",
    "        # Initialize weights and biases for forget gate\n",
    "        self.Wif = jax.random.normal(keys[3], (input_size, hidden_size)) * 0.01\n",
    "        self.Whf = jax.random.normal(keys[4], (hidden_size, hidden_size)) * 0.01\n",
    "        self.bf = jax.random.normal(keys[5], (hidden_size,)) * 0.01\n",
    "        \n",
    "        # Initialize weights and biases for cell gate\n",
    "        self.Wig = jax.random.normal(keys[6], (input_size, hidden_size)) * 0.01\n",
    "        self.Whg = jax.random.normal(keys[7], (hidden_size, hidden_size)) * 0.01\n",
    "        self.bg = jax.random.normal(keys[8], (hidden_size,)) * 0.01\n",
    "        \n",
    "        # Initialize weights and biases for output gate\n",
    "        self.Wio = jax.random.normal(keys[9], (input_size, hidden_size)) * 0.01\n",
    "        self.Who = jax.random.normal(keys[10], (hidden_size, hidden_size)) * 0.01\n",
    "        self.bo = jax.random.normal(keys[11], (hidden_size,)) * 0.01\n",
    "        \n",
    "        # Initialize default states (though these should typically be passed in)\n",
    "        self.h_0 = jnp.zeros((hidden_size,))\n",
    "        self.c_0 = jnp.zeros((hidden_size,))\n",
    "    \n",
    "    def get_initial_states(self):\n",
    "        \"\"\"Return the initial states.\"\"\"\n",
    "        return self.h_0, self.c_0\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(params, X_t, h_t, c_t):\n",
    "        \"\"\"\n",
    "        Forward pass of LSTM for a single time step.\n",
    "        All state is passed in explicitly for better compatibility with JAX.\n",
    "        \"\"\"\n",
    "        i = jax.nn.sigmoid(params['Wii'] @ X_t + params['Whi'] @ h_t + params['bi'])\n",
    "        f = jax.nn.sigmoid(params['Wif'] @ X_t + params['Whf'] @ h_t + params['bf'])\n",
    "        g = jax.nn.tanh(params['Wig'] @ X_t + params['Whg'] @ h_t + params['bg'])\n",
    "        o = jax.nn.sigmoid(params['Wio'] @ X_t + params['Who'] @ h_t + params['bo'])\n",
    "        \n",
    "        # Update cell state and hidden state\n",
    "        c_t_new = f * c_t + i * g\n",
    "        h_t_new = o * jax.nn.tanh(c_t_new)  \n",
    "\n",
    "        return c_t_new, h_t_new\n",
    "    \n",
    "    def params(self):\n",
    "        \"\"\"Return the parameters as a dictionary.\"\"\"\n",
    "        return {\n",
    "            'Wii': self.Wii, 'Whi': self.Whi, 'bi': self.bi,\n",
    "            'Wif': self.Wif, 'Whf': self.Whf, 'bf': self.bf,\n",
    "            'Wig': self.Wig, 'Whg': self.Whg, 'bg': self.bg,\n",
    "            'Wio': self.Wio, 'Who': self.Who, 'bo': self.bo,\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def totalLoss(params, x, y):\n",
    "        \"\"\"\n",
    "        Vectorized total loss function over all time steps using vmap.\n",
    "        \n",
    "        Parameters:\n",
    "        params: dict containing LSTM parameters\n",
    "        x: input sequence of shape (time_steps, input_size)\n",
    "        y: target sequence of shape (time_steps, hidden_size)\n",
    "        \n",
    "        Returns:\n",
    "        total loss across all time steps\n",
    "        \"\"\"\n",
    "        # Create a function that processes a single time step\n",
    "        def single_step(carry, inputs):\n",
    "            h_t, c_t = carry\n",
    "            x_t, y_t = inputs\n",
    "            \n",
    "            # Forward pass for single time step\n",
    "            c_t_new, h_t_new = LSTM.forward(params, x_t, h_t, c_t)  # Changed from forwardPass to forward\n",
    "            \n",
    "            # Compute loss for this time step\n",
    "            loss = jnp.sum((h_t_new - y_t)**2)\n",
    "            \n",
    "            return (h_t_new, c_t_new), loss\n",
    "\n",
    "        # Initialize carrying states\n",
    "        init_h = jnp.zeros_like(y[0])\n",
    "        init_c = jnp.zeros_like(y[0])\n",
    "        init_carry = (init_h, init_c)\n",
    "\n",
    "        # Use scan to process sequence\n",
    "        _, losses = jax.lax.scan(single_step, init_carry, (x, y))\n",
    "\n",
    "        # Sum up losses across all time steps\n",
    "        return jnp.sum(losses)\n",
    "    \n",
    "    def backward(self,xs,ys,epoch=10,lr=0.01):\n",
    "        Lossgrads = jax.value_and_grad(LSTM.totalLoss,argnums=0)\n",
    "        optimizer = optax.adam(learning_rate=lr)\n",
    "        opt_state = optimizer.init(self.params())\n",
    "        \n",
    "        for _ in range(epoch):\n",
    "            value,g = Lossgrads(params,xs,ys)\n",
    "            print(value)\n",
    "            updates, opt_state = optimizer.update(g, opt_state)\n",
    "            params = optax.apply_updates(params, updates)\n",
    "        \n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LSTM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m lstm \u001b[38;5;241m=\u001b[39m LSTM(input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, rng\u001b[38;5;241m=\u001b[39mjax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mkey(\u001b[38;5;241m0\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LSTM' is not defined"
     ]
    }
   ],
   "source": [
    "lstm = LSTM(input_size=10, hidden_size=20, rng=jax.random.key(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
